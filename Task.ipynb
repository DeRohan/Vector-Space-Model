{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-Processing the Research Papers and Stop Words.\n",
    "import glob as gb\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "document_list = defaultdict(str)\n",
    "directory = 'ResearchPapers/' #Directory Name\n",
    "originals = set() #List of all Document IDs to be used in Query Processing\n",
    "total_docs = 0\n",
    "#Reading all text files (research papers) in the directory\n",
    "for filename in gb.glob(directory + '*.txt'):\n",
    "    with open(filename, 'r', encoding='cp1252') as f:\n",
    "        filenum = re.search(r'\\d+', filename) #Getting only the decimal from the filename using regulax expression\n",
    "        if filenum:\n",
    "                document_list[int(filenum.group())] = (f.read().lower()) #Group function helps unpack the decimal value\n",
    "                originals.add(int(filenum.group()))\n",
    "                total_docs+=1\n",
    "\n",
    "stopwords = \"\"\n",
    "#Reading Stopwords Text File \n",
    "with open(\"Stopword-List.txt\", 'r') as f:\n",
    "        stopwords += f.read().strip() #Cleaning the String of any leading or trailing new line character\n",
    "stopwords = stopwords.replace(\" \", \"\") #Removing any extra whitespace characters\n",
    "stopwords = stopwords.replace('\\n', \" \") #Removing any new line character in between the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning data \n",
    "def clean_data(): #Gets rid of anything that is not in lowerecase alphabet set or a number\n",
    "    cleaned_data = defaultdict(str)\n",
    "    for docID in document_list:\n",
    "        string = document_list[docID]\n",
    "        cleaned_data[docID] = (re.sub(r'[^a-z]',\" \", string)) #Returns space seperated.\n",
    "    return cleaned_data\n",
    "\n",
    "document_list = clean_data() #Getting the cleaned data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization\n",
    "def tokenize(doc):\n",
    "    cleaned_doc = []\n",
    "    doc = list(doc.split()) #For whitespaced words\n",
    "    for word in doc:\n",
    "        if word not in stopwords: #Only including words that are not in our Stop Words File.\n",
    "            cleaned_doc.append(word)\n",
    "    return cleaned_doc\n",
    "\n",
    "token_list = defaultdict(list)\n",
    "for docID, doc in document_list.items():\n",
    "    cleaned_doc = tokenize(doc)\n",
    "    token_list[docID] = cleaned_doc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stemming\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer() #Using Built-in Stemmer\n",
    "\n",
    "stemmed_list = defaultdict(dict)\n",
    "for docID, doc in token_list.items():\n",
    "    stemmed_list[docID] = ([stemmer.stem(word) for word in doc]) #Creates a Stemmed List and Assigns it to its corresponding Document\n",
    "\n",
    "# stemmed_sw_list = defaultdict(list) \n",
    "# for docID, doc in tokens_with_stopword.items(): #Stemmed List including stopwords.\n",
    "#     stemmed_sw_list[docID] = [stemmer.stem(word) for word in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency & Document Frequency\n",
    "from math import log10\n",
    "term_occur = defaultdict(set)\n",
    "term_freq = defaultdict(lambda: defaultdict(int))\n",
    "for docID, doc in stemmed_list.items():\n",
    "    for i in doc:\n",
    "        term_freq[docID][i]+=1 #Term Frequency of each term in its corresponding document\n",
    "        term_occur[i].add(docID)\n",
    "\n",
    "doc_freq = {}\n",
    "for key, value in term_occur.items():\n",
    "    doc_freq[key] = len(value)\n",
    "    # print(f\"Document Frequency of {key} --> {doc_freq[key]}\")\n",
    "\n",
    "#Inverse Document Frequency\n",
    "idf = {}\n",
    "for key, value in doc_freq.items():\n",
    "    idf[key] = log10(total_docs/value)\n",
    "    # print(f\"IDF of {key} -> {idf[key]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverted Index\n",
    "inv_index = defaultdict(list)\n",
    "\n",
    "#TF-IDF Weighting\n",
    "tf_idf = defaultdict(lambda: defaultdict(float))\n",
    "for key, value in term_freq.items():\n",
    "    for term, freq in value.items():\n",
    "        tf_idf[term][key] = freq * idf[term]\n",
    "        inv_index[key].append(term)\n",
    "\n",
    "# print(inv_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building Vector Space Model\n",
    "vsm = defaultdict(lambda: defaultdict(float))\n",
    "for key, values in inv_index.items():\n",
    "    doc_vec = {}\n",
    "    for i in values:\n",
    "        doc_vec[i] = (tf_idf[i][key]) #Building Document Vector. At each term, its tf-idf weight is appended.\n",
    "    sorted_doc_vec = dict(sorted(doc_vec.items(), key=lambda x: x[1], reverse=True)) #Sorting the document vector using its tf-idf weighting\n",
    "    vsm[key] = sorted_doc_vec #Adding document vector to its document number.\n",
    "# print(vsm[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "from math import sqrt\n",
    "\n",
    "def cosine_similarity(query, document):\n",
    "    dot_prod = sum(query.get(term, 0) * document.get(term, 0) for term in set(query) and set(document))\n",
    "    query_mag = sqrt(sum(v**2 for v in query.values()))\n",
    "    doc_mag = sqrt(sum(v**2 for v in document.values()))\n",
    "    if query_mag == 0 or doc_mag == 0:\n",
    "        return 0\n",
    "    return dot_prod / (query_mag * doc_mag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to preprocess raw query to the algorithm specific format.\n",
    "from collections import Counter\n",
    "def preproccess_query(raw_query: str):\n",
    "    raw_query = raw_query.lower()\n",
    "    c_query = (re.sub(r'[^a-z]',\" \", raw_query))\n",
    "    tokens = tokenize(c_query)\n",
    "    refined_query = Counter([stemmer.stem(i) for i in tokens])\n",
    "    return refined_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 16 with Similarity: 0.04475412775866864\n",
      "Document ID: 24 with Similarity: 0.03972199017289384\n",
      "Document ID: 7 with Similarity: 0.038324706323010015\n",
      "Document ID: 2 with Similarity: 0.037076352127286806\n",
      "Document ID: 1 with Similarity: 0.030605522682982878\n",
      "Document ID: 3 with Similarity: 0.030322725304413174\n"
     ]
    }
   ],
   "source": [
    "def run_queries(query, alpha):\n",
    "    fine_query = preproccess_query(query)\n",
    "    ranked_documents = []\n",
    "    for docID, doc in vsm.items():\n",
    "        sim = cosine_similarity(fine_query, doc)\n",
    "        if sim >= alpha:\n",
    "            ranked_documents.append((docID, sim))\n",
    "    ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
    "    if len(ranked_documents):\n",
    "        return ranked_documents\n",
    "    return None\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    query = input(\"Enter your Query: \")\n",
    "    ranks = run_queries(query, 0.03)\n",
    "    if ranks is None:\n",
    "        print(\"No relevant documents found.\")\n",
    "    else: \n",
    "        for id, sim in ranks:\n",
    "            print(f\"Document ID: {id} with Similarity: {sim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-23 00:35:58.593 Python[8776:517989] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here2\n",
      "here2\n",
      "here3\n"
     ]
    }
   ],
   "source": [
    "#Driver Code including GUI\n",
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "\n",
    "class InformationRetrievalSearch(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title(\"Rohan's Retrieval\")\n",
    "        \n",
    "        # Set window size\n",
    "        self.geometry(\"800x600\")\n",
    "\n",
    "        # Variables to store input values\n",
    "        self.term1_prox_value = tk.StringVar(value=None)\n",
    "        self.term2_prox_value = tk.StringVar(value=None)\n",
    "        self.term1_value = tk.StringVar(value=None)\n",
    "        self.term2_value = tk.StringVar(value=None)\n",
    "        self.term3_value = tk.StringVar(value=None)\n",
    "        self.not1_value = tk.StringVar(value=None)\n",
    "        self.not2_value = tk.StringVar(value=None)\n",
    "        self.not3_value = tk.StringVar(value=None)\n",
    "        self.operator1_value = tk.StringVar(value=None)\n",
    "        self.operator2_value = tk.StringVar(value=None)\n",
    "        self.proximity_value = tk.StringVar(value=None)\n",
    "        self.result = set()\n",
    "        self.pos_result = set()\n",
    "        \n",
    "        # Create notebook to hold multiple pages\n",
    "        self.notebook = ttk.Notebook(self)\n",
    "        self.notebook.pack(expand=True, fill=tk.BOTH)\n",
    "        \n",
    "        #Building Inverted Index as soon as the GUI is Launched\n",
    "        self.inv_index = InvertedIndex(stemmed_list, originals)\n",
    "        self.inv_index.buildIndex()\n",
    "\n",
    "        #Building Positional Index\n",
    "        self.pos_index = PositionalIndex()\n",
    "        self.pos_index.buildIndex()\n",
    "\n",
    "        # Add inverted index page\n",
    "        self.add_inverted_index_page()\n",
    "\n",
    "        # Add positional index page\n",
    "        self.add_positional_index_page()\n",
    "    \n",
    "    #Processing Inverted Index Queries\n",
    "    def processInvertedIndexQuery(self, output_text):\n",
    "        index = self.inv_index\n",
    "        words = [self.term1_value.get().lower(), self.term2_value.get().lower(), self.term3_value.get().lower()]\n",
    "        \n",
    "        #Validating Inputs\n",
    "        if self.not1_value.get() == \"\":\n",
    "            self.not1_value.initialize(None)\n",
    "        if self.not2_value.get() == \"\":\n",
    "            self.not2_value.initialize(None)\n",
    "        if self.not3_value.get() == \"\":\n",
    "            self.not3_value.initialize(None)\n",
    "        if self.operator1_value.get() == \"\":\n",
    "            self.operator1_value.initialize(None)\n",
    "        if self.operator2_value.get() == \"\":\n",
    "            self.operator2_value.initialize(None)\n",
    "\n",
    "        if words[0] == \"Enter term 1\":\n",
    "            words[0] = None\n",
    "        if words[1] == \"Enter term 2\":\n",
    "            words[1] = None\n",
    "        if words[2] == \"Enter term 3\":\n",
    "            words[2] = None\n",
    "\n",
    "        #Query Processing\n",
    "        self.result = index.processQueries(words, self.not1_value.get(), self.not2_value.get(), self.not3_value.get(), self.operator1_value.get(), self.operator2_value.get())\n",
    "        self.display_result(self.result, output_text)\n",
    "\n",
    "    #Inverted Index Page\n",
    "    def add_inverted_index_page(self):\n",
    "        inverted_index_frame = tk.Frame(self.notebook)\n",
    "        \n",
    "        #Dropdown for NOT operator before term 1\n",
    "        not_dropdown1 = ttk.Combobox(inverted_index_frame, values=[\"\", \"NOT\"], width=5, textvariable=self.not1_value)\n",
    "        not_dropdown1.pack(pady=5)\n",
    "        \n",
    "        #Term 1 input\n",
    "        term1_entry = tk.Entry(inverted_index_frame, width=30, textvariable=self.term1_value)\n",
    "        term1_entry.insert(0, \"Enter term 1\")\n",
    "        term1_entry.bind(\"<FocusIn>\", lambda event: self.clear_placeholder(event, term1_entry))\n",
    "        term1_entry.pack(pady=5)\n",
    "        \n",
    "        #Dropdown between term 1 and term 2\n",
    "        operator_dropdown1 = ttk.Combobox(inverted_index_frame, values=[\"\",\"AND\", \"OR\"], width=5, textvariable=self.operator1_value)\n",
    "        operator_dropdown1.pack(pady=5)\n",
    "        \n",
    "        #Dropdown for NOT operator before term 2\n",
    "        not_dropdown2 = ttk.Combobox(inverted_index_frame, values=[\"\", \"NOT\"], width=5, textvariable=self.not2_value)\n",
    "        not_dropdown2.pack(pady=5)\n",
    "        \n",
    "        # Term 2 input\n",
    "        term2_entry = tk.Entry(inverted_index_frame, width=30, textvariable=self.term2_value)\n",
    "        term2_entry.insert(0, \"Enter term 2\")\n",
    "        term2_entry.bind(\"<FocusIn>\", lambda event: self.clear_placeholder(event, term2_entry))\n",
    "        term2_entry.pack(pady=5)\n",
    "        \n",
    "        #Dropdown between term 2 and term 3\n",
    "        operator_dropdown2 = ttk.Combobox(inverted_index_frame, values=[\"\",\"AND\", \"OR\"], width=5, textvariable=self.operator2_value)\n",
    "        operator_dropdown2.pack(pady=5)\n",
    "\n",
    "        #Dropdown for NOT operator before term 3\n",
    "        not_dropdown1 = ttk.Combobox(inverted_index_frame, values=[\"\", \"NOT\"], width=5, textvariable=self.not3_value)\n",
    "        not_dropdown1.pack(pady=5)\n",
    "        \n",
    "        #Term 3 input\n",
    "        term3_entry = tk.Entry(inverted_index_frame, width=30, textvariable=self.term3_value)\n",
    "        term3_entry.insert(0, \"Enter term 3\")\n",
    "        term3_entry.bind(\"<FocusIn>\", lambda event: self.clear_placeholder(event, term3_entry))\n",
    "        term3_entry.pack(pady=5)\n",
    "        \n",
    "        \n",
    "        # Search button\n",
    "        inverted_search_button = tk.Button(inverted_index_frame, text=\"Search\", width=20, command=lambda: self.processInvertedIndexQuery(output_text))\n",
    "        inverted_search_button.pack(pady=10)\n",
    "\n",
    "        #Heading for Output Box\n",
    "        output_label = tk.Label(inverted_index_frame, text=\"Retrieved Documents\")\n",
    "        output_label.pack(pady=5)\n",
    "\n",
    "        #Output box\n",
    "        output_text = tk.Text(inverted_index_frame, height=10, width=70)\n",
    "        output_text.pack(pady=15)\n",
    "        output_text.insert(tk.END, \"This is a sample result.\")\n",
    "        output_text.config(state=\"disabled\")\n",
    "\n",
    "        self.notebook.add(inverted_index_frame, text=\"Boolean Queries\")\n",
    "\n",
    "    def processProximityQueries(self, output_text):\n",
    "            words = [self.term1_prox_value.get().lower(), self.term2_prox_value.get().lower()]\n",
    "            distance = int(self.proximity_value.get())\n",
    "            pos_index = self.pos_index\n",
    "            self.pos_result = pos_index.processQuery(words, distance)\n",
    "\n",
    "            self.display_result(self.pos_result, output_text)\n",
    "\n",
    "    #Positional Index Page\n",
    "    def add_positional_index_page(self):\n",
    "        positional_index_frame = tk.Frame(self.notebook)\n",
    "        \n",
    "        #Search bars for term 1 and term 2\n",
    "        positional_search_entry_1 = tk.Entry(positional_index_frame, width=50, textvariable=self.term1_prox_value)\n",
    "        positional_search_entry_1.insert(0, \"Enter term 1\")\n",
    "        positional_search_entry_1.bind(\"<FocusIn>\", lambda event: self.clear_placeholder(event, positional_search_entry_1))\n",
    "        positional_search_entry_1.pack(pady=5)\n",
    "        \n",
    "        positional_search_entry_2 = tk.Entry(positional_index_frame, width=50, textvariable=self.term2_prox_value)\n",
    "        positional_search_entry_2.insert(0, \"Enter term 2\")\n",
    "        positional_search_entry_2.bind(\"<FocusIn>\", lambda event: self.clear_placeholder(event, positional_search_entry_2))\n",
    "        positional_search_entry_2.pack(pady=5)\n",
    "\n",
    "        #Entry for proximity integer value\n",
    "        proximity_entry = tk.Entry(positional_index_frame, width=50, textvariable=self.proximity_value)\n",
    "        proximity_entry.insert(0, \"Enter proximity value\")\n",
    "        proximity_entry.bind(\"<FocusIn>\", lambda event: self.clear_placeholder(event, proximity_entry))\n",
    "        proximity_entry.pack(pady=5)\n",
    "        \n",
    "        #Search button\n",
    "        positional_search_button = tk.Button(positional_index_frame, text=\"Search\", width=20, command=lambda: self.processProximityQueries(output_text))\n",
    "        positional_search_button.pack(pady=10)\n",
    "        \n",
    "        #Heading for output box\n",
    "        output_label = tk.Label(positional_index_frame, text=\"Retrieved Documents\")\n",
    "        output_label.pack(pady=5)\n",
    "\n",
    "        #Output box\n",
    "        output_text = tk.Text(positional_index_frame, height=10, width=60)\n",
    "        output_text.pack(pady=10)\n",
    "        output_text.insert(tk.END, \"This is a sample result.\")\n",
    "        output_text.config(state=\"disabled\")\n",
    "\n",
    "\n",
    "        self.notebook.add(positional_index_frame, text=\"Proximity Queries\")\n",
    "\n",
    "    def clear_placeholder(self, event, entry_widget):\n",
    "        if entry_widget.get() == \"Enter term 1\" or entry_widget.get() == \"Enter term 2\" or entry_widget.get() == \"Enter term 3\" or entry_widget.get() == \"Enter proximity value\":\n",
    "            entry_widget.delete(0, tk.END)\n",
    "\n",
    "    def display_result(self, result, output_text):\n",
    "        #Get search query and perform search\n",
    "        if result:\n",
    "            result_str = \"\\n\".join(map(str, result))\n",
    "        else:\n",
    "            result_str = \"No Documents Found....\"\n",
    "        \n",
    "        #Display result in the output box\n",
    "        output_text.config(state=\"normal\")\n",
    "        output_text.delete(\"1.0\", tk.END)  #Clear previous content\n",
    "        output_text.insert(tk.END, result_str)\n",
    "        output_text.config(state=\"disabled\")\n",
    "\n",
    "#Main Driver Code\n",
    "if __name__ == \"__main__\":\n",
    "    app = InformationRetrievalSearch()\n",
    "    app.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
